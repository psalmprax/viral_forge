import torch
import os, uuid, logging, imageio, numpy as np
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from diffusers import DiffusionPipeline
import uvicorn
import subprocess
from pathlib import Path

# --- FISH SPEECH IMPORTS (Dynamic) ---
try:
    from fish_speech.utils.schema import TTSRequest
    from fish_speech.inference import TTSInference
except ImportError:
    TTSInference = None

app = FastAPI()

def load_pipeline():
    print("‚è≥ Loading LTX-Video... (This takes 5-10 minutes)")
    # Using DiffusionPipeline is the recommended way
    pipe = DiffusionPipeline.from_pretrained(
        "Lightricks/LTX-Video", 
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True
    )
    # REQUIRED: This enables sub-16GB memory management
    # For Higher Stability on 12-16GB GPUs use sequential_cpu_offload
    # For performance on 24GB+ use model_cpu_offload
    if torch.cuda.get_device_properties(0).total_memory < 20 * 1024**3:
        print("Using Sequential CPU Offload for low VRAM stability")
        pipe.enable_sequential_cpu_offload()
    else:
        print("Using Model CPU Offload for performance")
        pipe.enable_model_cpu_offload()
        
    return pipe

# Global inference engines
pipe = None
tts_engine = None

def load_tts():
    print("‚è≥ Loading Fish Speech 1.5...")
    # This assumes fish-speech is installed in the environment
    # On Colab we will need a !pip install fish-speech
    try:
        engine = TTSInference(model_checkpoint="models/fish-speech-1.5")
        if torch.cuda.is_available():
            engine.to("cuda")
        return engine
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load TTS: {e}")
        return None

class GenerationRequest(BaseModel):
    prompt: str
    duration_seconds: int = 5

@app.post("/generate")
async def generate(req: GenerationRequest, bg: BackgroundTasks):
    global pipe
    if pipe is None:
        pipe = load_pipeline()
        
    job_id = f"render_{uuid.uuid4().hex[:8]}"
    bg.add_task(render_task, job_id, req.prompt)
    return {"job_id": job_id, "download_url": f"/download/{job_id}"}

class VoiceRequest(BaseModel):
    text: str

@app.post("/voice/generate")
async def generate_voice(req: VoiceRequest):
    global tts_engine
    if tts_engine is None:
        tts_engine = load_tts()
    
    if tts_engine is None:
        return {"error": "TTS Engine failed to load"}

    job_id = f"voice_{uuid.uuid4().hex[:8]}"
    audio_path = f"/content/{job_id}.mp3"
    
    # Run inference (simplified for the node)
    tts_engine.generate(req.text, output_path=audio_path)
    
    return {"job_id": job_id, "download_url": f"/download_audio/{job_id}"}

@app.get("/download_audio/{job_id}")
async def download_audio(job_id: str):
    from fastapi.responses import FileResponse
    path = f"/content/{job_id}.mp3"
    if os.path.exists(path):
        return FileResponse(path)
    return {"error": "Not found"}

def render_task(job_id, prompt):
    print(f"üé¨ Starting generation for: {prompt}")
    try:
        with torch.inference_mode():
            # Memory-safe generation settings
            frames = pipe(
                prompt=prompt, 
                num_frames=121, 
                decode_chunk_size=8,
                num_inference_steps=30
            ).frames[0]
            
        output_path = f"/content/{job_id}.mp4"
        writer = imageio.get_writer(output_path, fps=24)
        for frame in frames:
            writer.append_data(np.array(frame))
        writer.close()
        print(f"‚úÖ Finished: {output_path}")
    except Exception as e:
        print(f"‚ùå Error: {e}")

@app.get("/download/{job_id}")
async def download(job_id: str):
    from fastapi.responses import FileResponse
    path = f"/content/{job_id}.mp4"
    if os.path.exists(path):
        return FileResponse(path)
    return {"error": "Not found"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
